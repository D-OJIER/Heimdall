<!doctype html>
<html>
<head>
  <meta charset="utf-8" />
  <title>YOLOv5 Live Client</title>
  <style>
    body { font-family: Arial, sans-serif; margin: 16px; }
    video, canvas, img { border: 1px solid #ccc; max-width: 640px; display: block; margin-bottom: 8px; }
    #controls { margin-bottom: 12px; }
    #detections { white-space: pre-wrap; font-family: monospace; }
  </style>
</head>
<body>
  <h2>YOLOv5 Live Client</h2>
  <div id="controls">
    <button id="startBtn">Start</button>
    <button id="stopBtn" disabled>Stop</button>
    <label>FPS: <input id="fps" type="number" value="5" min="1" max="30" style="width:60px"/></label>
    <label>WS URL: <input id="wsUrl" type="text" value="ws://localhost:8001/ws/live" style="width:320px"/></label>
  </div>
  <div id="viewer" style="position:relative; display:inline-block">
    <video id="video" autoplay muted playsinline width="640" height="480" style="display:block"></video>
    <!-- overlay canvas drawn client-side using normalized detections from server -->
    <canvas id="overlay" width="640" height="480" style="position:absolute; left:0; top:0; pointer-events:none"></canvas>
  </div>
  <canvas id="canvas" width="640" height="480" style="display:none"></canvas>
  <h3>Annotated (server JPEG, optional) -></h3>
  <img id="annotated" alt="annotated frame" style="max-width:320px" />
  <h3>Detections</h3>
  <div id="detections">(none)</div>

  <script>
    const startBtn = document.getElementById('startBtn')
    const stopBtn = document.getElementById('stopBtn')
    const video = document.getElementById('video')
    const canvas = document.getElementById('canvas')
  const annotated = document.getElementById('annotated')
    const detectionsDiv = document.getElementById('detections')
    const fpsInput = document.getElementById('fps')
    const wsUrlInput = document.getElementById('wsUrl')
  const overlay = document.getElementById('overlay')
  const overlayCtx = overlay.getContext('2d')

    let ws = null
    let stream = null
    let sendInterval = null

    async function start() {
      try {
        stream = await navigator.mediaDevices.getUserMedia({video: true, audio: false})
        video.srcObject = stream
        await video.play()
      } catch (e) {
        alert('Failed to open webcam: ' + e)
        return
      }

      const wsUrl = wsUrlInput.value
      ws = new WebSocket(wsUrl)
      ws.onopen = () => {
        console.log('ws open')
        startBtn.disabled = true
        stopBtn.disabled = false
        const fps = Math.max(1, parseInt(fpsInput.value || '5'))
        const interval = 1000 / fps
        sendInterval = setInterval(sendFrame, interval)
      }
      ws.onmessage = (ev) => {
        try {
          const data = JSON.parse(ev.data)
          // server-provided annotated JPEG shown as small preview (optional)
          if (data.annotated_b64) annotated.src = data.annotated_b64

          // draw client-side overlay for lowest-latency boxes on top of the video
          if (data.detections) {
            renderOverlay(data.detections)
            if (data.detections.length === 0) detectionsDiv.textContent = '(no detections)'
            else {
              const lines = data.detections.map(d => `${d.label} ${d.score.toFixed(2)} [${(d.xmin*100).toFixed(1)}%, ${(d.ymin*100).toFixed(1)}% -> ${(d.xmax*100).toFixed(1)}%, ${(d.ymax*100).toFixed(1)}%]`)
              detectionsDiv.textContent = lines.join('\n')
            }
          }
        } catch (e) { console.warn('invalid ws message', e) }
      }
      ws.onclose = () => { console.log('ws closed'); stop(); }
      ws.onerror = (e) => { console.error('ws error', e); }
    }

    function stop() {
      if (sendInterval) { clearInterval(sendInterval); sendInterval = null }
      if (ws) { ws.close(); ws = null }
      if (stream) {
        stream.getTracks().forEach(t => t.stop())
        stream = null
      }
      startBtn.disabled = false
      stopBtn.disabled = true
    }

    function sendFrame() {
      if (!ws || ws.readyState !== WebSocket.OPEN) return
      const ctx = canvas.getContext('2d')
      ctx.drawImage(video, 0, 0, canvas.width, canvas.height)
      // compress to jpeg data URL
      const dataUrl = canvas.toDataURL('image/jpeg', 0.8)
      const payload = { frame_id: `frame-${Date.now()}`, image_b64: dataUrl }
      ws.send(JSON.stringify(payload))
    }

    function renderOverlay(detections) {
      // overlay canvas is same size as video; detections are normalized
      const w = overlay.width
      const h = overlay.height
      overlayCtx.clearRect(0, 0, w, h)
      overlayCtx.lineWidth = 3
      overlayCtx.font = '16px Arial'
      overlayCtx.textBaseline = 'top'
      detections.forEach(d => {
        const x1 = Math.round(d.xmin * w)
        const y1 = Math.round(d.ymin * h)
        const x2 = Math.round(d.xmax * w)
        const y2 = Math.round(d.ymax * h)
        const bw = Math.max(2, Math.round(2 * (w / 640)))
        overlayCtx.lineWidth = bw
        overlayCtx.strokeStyle = 'rgba(255,0,0,0.9)'
        overlayCtx.fillStyle = 'rgba(255,0,0,0.9)'
        overlayCtx.strokeRect(x1, y1, x2 - x1, y2 - y1)
        const label = `${d.label} ${d.score.toFixed(2)}`
        const textW = overlayCtx.measureText(label).width
        const pad = 4
        overlayCtx.fillRect(x1, Math.max(0, y1 - 20), textW + pad * 2, 20)
        overlayCtx.fillStyle = 'white'
        overlayCtx.fillText(label, x1 + pad, Math.max(0, y1 - 18))
      })
    }

    startBtn.addEventListener('click', start)
    stopBtn.addEventListener('click', stop)

    // clean up on page unload
    window.addEventListener('beforeunload', () => { stop() })
  </script>
</body>
</html>